{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2788,"status":"ok","timestamp":1709598607675,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"4PRyrcZHjTnM","outputId":"fa74e63a-10c8-4e1b-8848-ec4602e24cc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.15.0\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1709598608122,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"D3zahgSwkKSt","outputId":"883d91f1-dd20-4be9-d1d6-51da4794806b"},"outputs":[{"data":{"text/plain":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["tf.config.list_physical_devices('GPU')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2248,"status":"ok","timestamp":1709598610367,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"QGkv6tcDjXZi","outputId":"64fcb25d-488a-4269-a027-1e743001bb1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 2s 0us/step\n"]}],"source":["# loading dataset\n","mnist = tf.keras.datasets.mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":395,"status":"ok","timestamp":1709598610761,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"wA4lUyjmjYhk"},"outputs":[],"source":["input_shape = (28, 28, 1)\n","\n","x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n","x_train = x_train / 255.0\n","x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n","x_test = x_test / 255.0"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1709598610761,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"g6ND_fMsjaQ_"},"outputs":[],"source":["# one hot encoding\n","# y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\n","y_test = tf.one_hot(y_test.astype(np.int32), depth=10)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1709598610762,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"XHX9l4WujcXq"},"outputs":[],"source":["record_defaults = [tf.constant([], dtype=tf.float32)] * 10\n","\n","y_train = tf.data.experimental.CsvDataset(\n","    filenames=[\"/content/drive/MyDrive/noisy_y_train.csv\"], record_defaults=record_defaults\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":50620,"status":"ok","timestamp":1709598661379,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"Mm2z2onLjeY1"},"outputs":[],"source":["# Convert the CsvDataset to a list of EagerTensors\n","y_train = [tf.convert_to_tensor(value) for value in y_train]"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1709598661380,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"V2882VqcjgAJ","outputId":"37f70601-edbb-4563-e9df-e00ea63c9d70"},"outputs":[{"data":{"text/plain":["60000"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["len(y_train)"]},{"cell_type":"markdown","metadata":{"id":"FiDMEOhhjkDU"},"source":["Getting model and all original hyperparameters"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1709598661380,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"JGDxGzBnjhvI"},"outputs":[],"source":["batch_size = 64\n","num_classes = 10\n","num_epochs = 30"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1709598661380,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"NT2XjTbCjjUp"},"outputs":[],"source":["optimizer = tf.keras.optimizers.SGD(name=\"SGD\")\n","cce = tf.keras.losses.CategoricalCrossentropy()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1709598661380,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"4LD1cWoejma5"},"outputs":[],"source":["model = tf.keras.models.Sequential(\n","    [\n","        tf.keras.layers.Conv2D(32,(5, 5), padding=\"same\", activation=\"relu\", input_shape=input_shape, name=\"conv1_1\"),\n","        tf.keras.layers.Conv2D(32, (5, 5), padding=\"same\", activation=\"relu\", name=\"conv1_2\"),\n","        tf.keras.layers.MaxPool2D(),\n","        tf.keras.layers.Dropout(0.25),\n","        tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv2_1\"),\n","        tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv2_2\"),\n","        tf.keras.layers.MaxPool2D(strides=(2, 2)),\n","        tf.keras.layers.Dropout(0.25),\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dense(128, activation=\"relu\", name=\"FC_1\"),\n","        tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(128, activation=\"relu\", name=\"FC_2\"),\n","        tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"FC_softmax\"),\n","    ]\n",")\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.SGD(name=\"SGD\"),\n","    loss=\"categorical_crossentropy\",\n","    metrics=[\"acc\"],\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2304,"status":"ok","timestamp":1709598663673,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"hDeQXYfkjqXg","outputId":"d2a38210-5f98-4e81-a02d-683353b3cd05"},"outputs":[{"data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7cd11414db40>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Load gradients from the checkpoint file\n","checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n","checkpoint.restore(\"/content/drive/MyDrive/epoch_layer_gradients.ckpt-1\").expect_partial()"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":817,"status":"ok","timestamp":1709598664481,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"SKgGhYrwjrzA"},"outputs":[],"source":["# get indices with noisy data\n","indices = (np.loadtxt(\"/content/drive/MyDrive/noisy_indices.csv\", delimiter=\",\")).astype(int)\n","indices = indices.tolist()\n","\n","# convert eagertensor to numpy\n","# y_train_numpy = [value.numpy() for value in y_train]\n","\n","# create a x_batch of 1000 values using the 6k noisy examples\n","noisy_values_y = [y_train[i] for i in indices]\n","noisy_sample_y = noisy_values_y[:6000:6]\n","\n","noisy_values_x = [x_train[i] for i in indices]\n","noisy_sample_x = noisy_values_x[:6000:6]"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2541,"status":"ok","timestamp":1709598667603,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"PnKw8sYgjuVm"},"outputs":[],"source":["# get clean indices\n","total_indices = list(range(60000))\n","clean_indices = [value for value in total_indices if value not in indices]\n","\n","# create a sample of 1000 values using the remaining 54k clean examples\n","clean_values_y = [y_train[i] for i in clean_indices]\n","clean_sample_y = clean_values_y[:54000:54]\n","\n","clean_values_x = [x_train[i] for i in clean_indices]\n","clean_sample_x = clean_values_x[:54000:54]"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":1437,"status":"ok","timestamp":1709598669484,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"6lZyLOnKjvu3"},"outputs":[],"source":["noisy_sample_x = tf.convert_to_tensor(noisy_sample_x)\n","noisy_sample_y = tf.convert_to_tensor(noisy_sample_y)\n","clean_sample_x = tf.convert_to_tensor(clean_sample_x)\n","clean_sample_y = tf.convert_to_tensor(clean_sample_y)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1709598669484,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"jBTO_gHpjyye","outputId":"cfe78fdd-bcc8-4e6b-ac60-3cb23b8d9bf2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\t noisy_sample \t clean_sample \t type\n","x shape (28, 28, 1) \t (28, 28, 1) \t <class 'tensorflow.python.framework.ops.EagerTensor'>\n","y shape (10,) \t \t (10,) \t \t <class 'tensorflow.python.framework.ops.EagerTensor'>\n","total <class 'tensorflow.python.framework.ops.EagerTensor'> 1000\n","total <class 'tensorflow.python.framework.ops.EagerTensor'> 1000\n","total <class 'tensorflow.python.framework.ops.EagerTensor'> 1000\n","total <class 'tensorflow.python.framework.ops.EagerTensor'> 1000\n"]}],"source":["print(\"\\t\",\"noisy_sample\",\"\\t\", \"clean_sample\", \"\\t\",\"type\")\n","print(\"x shape\" , noisy_sample_x[0].shape,\"\\t\",clean_sample_x[0].shape,\"\\t\", type(noisy_sample_x[0]))\n","print(\"y shape\" , noisy_sample_y[0].shape,\"\\t\",\"\\t\",clean_sample_y[0].shape, \"\\t\",\"\\t\",type(clean_sample_y[0]))"]},{"cell_type":"markdown","metadata":{"id":"hvq8SxhQj3AU"},"source":["Get gradients for each element: &#8711;$_l (loss(F_d(x_i), y_i))$"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57276,"status":"ok","timestamp":1709598732765,"user":{"displayName":"Ayesha Ansar","userId":"08341204205558293174"},"user_tz":-60},"id":"tBm0PUwtA-70","outputId":"57cdb571-b726-45d1-91ab-22757bb9bf59"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x7cd1140be320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x7cd1140be320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]}],"source":["\n","batch_size = 32\n","\n","# comment out as needed\n","sample_x = noisy_sample_x\n","sample_y = noisy_sample_y\n","\n","# sample_x = clean_sample_x\n","# sample_y = clean_sample_y\n","\n","\n","individual_gradients = []\n","for batch_start in range(1000):\n","    x_batch = sample_x[batch_start:batch_start+batch_size]\n","    y_batch = sample_y[batch_start:batch_start+batch_size]\n","\n","    with tf.GradientTape() as tape:\n","        tape.watch(model.trainable_variables)\n","        pred = model(x_batch)\n","        loss = cce(pred, y_batch)\n","        # loss_val = tf.reduce_mean(loss)\n","\n","    indv_grad = tape.jacobian(loss, model.trainable_variables)\n","    individual_gradients.append(indv_grad)"]},{"cell_type":"markdown","metadata":{},"source":["Get gradients of batches: &#8711;$_l L(S,F_d))$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch_size = 32\n","batch_gradients = []\n","\n","for batch_start in range(0, 1000, batch_size):\n","    x_batch = sample_x[batch_start:batch_start+batch_size]\n","    y_batch = sample_y[batch_start:batch_start+batch_size]\n","\n","    with tf.GradientTape() as tape:\n","        tape.watch(model.trainable_variables)\n","        pred = model(x_batch)\n","        loss = cce(pred, y_batch)\n","        loss_val = tf.reduce_mean(loss)\n","\n","    batch_grad = tape.gradient(loss_val, model.trainable_variables)\n","    batch_gradients.append(batch_grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yXa1yB2qpuV_"},"outputs":[],"source":["import pickle\n","# comment out as needed\n","\n","# file_path = '/content/drive/MyDrive/individual_gradients_clean.pkl'\n","# with open(file_path, 'wb') as file:\n","#     pickle.dump(individual_gradients, file)\n","    \n","file_path = '/content/drive/MyDrive/individual_gradients_noisy.pkl'\n","with open(file_path, 'wb') as file:\n","    pickle.dump(individual_gradients, file)"]},{"cell_type":"markdown","metadata":{},"source":["Get a list of neurons for each example"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["example_to_neuron = []\n","for batch_idx, batch in enumerate(batch_gradients):\n","    print(\"Batch Index:\", batch_idx)\n","    for ex_idx in range(batch_idx * 32, (batch_idx + 1) * 32):\n","\n","      if ex_idx >= 1000:\n","        break\n","      \n","      # compute derivatives of (small l(F_d(x_i), y_i) - L(S,F_d))\n","      difference = [tf.subtract(t1, t2) for t1, t2 in zip(individual_gradients[ex_idx], batch)]\n","\n","      neuron_of_layer = []\n","      for layer_idx, layer in enumerate(difference):\n","        norms = []\n","\n","        for neuron_idx in range(layer.shape[-1]):\n","\n","          if layer_idx in {0, 2, 4, 6}:\n","            norm = tf.norm(layer[:, :, :, neuron_idx])\n","            norms.append(norm)\n","          elif layer_idx in {8, 10}:\n","            norm = tf.norm(layer[:, neuron_idx])\n","            norms.append(norm)\n","          else:\n","            continue\n","        # print(\"layer index:\", layer_idx, \"len of norms:\", len(norms))\n","\n","        if layer_idx in {1,3,5,7,9,11,12,13}:\n","          continue\n","        # Extract the index with the highest norm\n","        imp_norm = tf.argmax(norms)\n","        neuron_of_layer.append(imp_norm)\n","\n","      example_to_neuron.append(neuron_of_layer)\n","\n","# Access the result as needed\n","print(\"Example to Neuron Mapping:\")\n","print(example_to_neuron)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","# comment as needed\n","\n","# file_path = '/content/drive/MyDrive/examples_to_neurons_clean.pkl'\n","# with open(file_path, 'wb') as file:\n","#     pickle.dump(example_to_neuron, file)\n","    \n","file_path = '/content/drive/MyDrive/examples_to_neurons_noisy.pkl'\n","with open(file_path, 'wb') as file:\n","    pickle.dump(example_to_neuron, file)\n","    \n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPKnZUJnCAAy2EAkQfodvKA","gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
